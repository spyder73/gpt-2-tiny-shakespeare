# GPT-2 Decoder — Implementation Notes

This ReadMe is an AI-enhanced version of my notes i used to understand + code this. 
It is my implementation of a GPT-2 style decoder-only transformer in PyTorch, based on the "Attention is All You Need" paper.
I use `tiktoken` as the tokenizer and built everything else myself with PyTorch.
---

## Architecture Overview

The model is a **decoder-only transformer**, meaning there is no encoder and no cross-attention — just stacked masked self-attention blocks.

Each block consists of:
1. **Masked Multi-Head Self-Attention**
2. **Add & Norm** (residual connection + LayerNorm)
3. **Feed-Forward Network** (FFN)
4. **Add & Norm**

The model parameters are:
- `n_layers` — number of stacked AttentionBlock + FFN + LayerNorm blocks
- `n_heads` — number of attention heads per Multi-Head Attention layer
- `d_model` — embedding dimension, also `d_q = d_k = d_v` per head (so `head_size = d_model / n_heads`) - "richness of features per token"

All parameters from the paper are used where applicable, except the model is scaled down for training on a weaker GPU. There are two options for the attention mechanism — my own implementation via explicit matrix multiplications, or PyTorch's built-in `nn.MultiheadAttention`.

---

## Dataset & Training

Trained on the **tiny Shakespeare dataset**. Since the dataset is quite small, overfitting happens fast — but text generation works surprisingly well for what it is. The model most likely learns to memorize rather than truly generalize, which is expected at this scale.

The **sequence length** defines the context window fed into the decoder. The paper uses a max of 1024 tokens; here we use 32–128. In each training step, a random batch is generated by sampling random indices from the dataset and taking `seq_length` following tokens as the input sequence.

The **learning rate schedule** follows the formula from the paper:

$$lr = d_{model}^{-0.5} \cdot \min\left(step^{-0.5},\ step \cdot warmup\_steps^{-1.5}\right)$$

---

## Positional Encoding

### The Problem

Self-attention is **permutation invariant** — the dot products $Q_i \cdot K_j$ don't care about where tokens sit in the sequence, only about the content of their embeddings. Without positional encoding, "Shakespeare wrote Hamlet" and "Hamlet wrote Shakespeare" look identical to the model — just a bag of tokens.

We fix this by **adding** a fixed positional encoding vector to each token's embedding before it enters the attention layers.

### The Formula

For each position $pos$ in the sequence and each dimension $i$ of the embedding:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i / d_{model}}}\right) \qquad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i / d_{model}}}\right)$$

Even dimensions get sine, odd dimensions get cosine. Each pair of dimensions oscillates at a different frequency — fast for early dimensions (local position info), slow for later dimensions (coarse position info). Like a binary counter in continuous space.

### Concrete Example — How It's Added

Take a 3-token sequence `"The cat sat"` with $d_{model} = 8$. The embedding layer gives each token a learned vector:

$$E_{\text{The}} = [\alpha_1,\ \alpha_2,\ \alpha_3,\ \alpha_4,\ \alpha_5,\ \alpha_6,\ \alpha_7,\ \alpha_8]$$
$$E_{\text{cat}} = [\beta_1,\ \beta_2,\ \beta_3,\ \beta_4,\ \beta_5,\ \beta_6,\ \beta_7,\ \beta_8]$$
$$E_{\text{sat}} = [\gamma_1,\ \gamma_2,\ \gamma_3,\ \gamma_4,\ \gamma_5,\ \gamma_6,\ \gamma_7,\ \gamma_8]$$

These embeddings contain **semantic** information but nothing about position. Now the positional encoding for each position. With $d_{model} = 8$ we have 4 frequency pairs with $\omega_i = 1/10000^{2i/8}$:

$$PE_0 = [\underbrace{\sin(0),\ \cos(0)}_{\omega_0},\ \underbrace{\sin(0),\ \cos(0)}_{\omega_1},\ \underbrace{\sin(0),\ \cos(0)}_{\omega_2},\ \underbrace{\sin(0),\ \cos(0)}_{\omega_3}]\ = [0,\ 1,\ 0,\ 1,\ 0,\ 1,\ 0,\ 1]$$

$$PE_1 = [\underbrace{\sin(\omega_0),\ \cos(\omega_0)}_{\omega_0},\ \underbrace{\sin(\omega_1),\ \cos(\omega_1)}_{\omega_1},\ \underbrace{\sin(\omega_2),\ \cos(\omega_2)}_{\omega_2},\ \underbrace{\sin(\omega_3),\ \cos(\omega_3)}_{\omega_3}]$$

$$PE_2 = [\underbrace{\sin(2\omega_0),\ \cos(2\omega_0)}_{\omega_0},\ \underbrace{\sin(2\omega_1),\ \cos(2\omega_1)}_{\omega_1},\ \underbrace{\sin(2\omega_2),\ \cos(2\omega_2)}_{\omega_2},\ \underbrace{\sin(2\omega_3),\ \cos(2\omega_3)}_{\omega_3}]$$

Each pair of dims is a $(\sin, \cos)$ evaluated at $pos \cdot \omega_i$. The fast frequencies $\omega_0$ change rapidly between positions, the slow frequencies $\omega_3$ barely move.

We **add element-wise** — each dimension of the embedding gets a different positional shift depending on which position the token is at:

$$\text{input}_{\text{The}} = E_{\text{The}} + PE_0 = [\alpha_1 + 0,\ \alpha_2 + 1,\ \alpha_3 + 0,\ \alpha_4 + 1,\ \alpha_5 + 0,\ \alpha_6 + 1,\ \alpha_7 + 0,\ \alpha_8 + 1]$$

$$\text{input}_{\text{cat}} = E_{\text{cat}} + PE_1 = [\beta_1 + \sin(\omega_0),\ \beta_2 + \cos(\omega_0),\ \beta_3 + \sin(\omega_1),\ \beta_4 + \cos(\omega_1),\ \beta_5 + \sin(\omega_2),\ \beta_6 + \cos(\omega_2),\ \beta_7 + \sin(\omega_3),\ \beta_8 + \cos(\omega_3)]$$

$$\text{input}_{\text{sat}} = E_{\text{sat}} + PE_2 = [\gamma_1 + \sin(2\omega_0),\ \gamma_2 + \cos(2\omega_0),\ \gamma_3 + \sin(2\omega_1),\ \gamma_4 + \cos(2\omega_1),\ \gamma_5 + \sin(2\omega_2),\ \gamma_6 + \cos(2\omega_2),\ \gamma_7 + \sin(2\omega_3),\ \gamma_8 + \cos(2\omega_3)]$$

The same word at **different positions** would get a different input vector — each dimension gets nudged by a different amount dictated purely by the position. The model learns to disentangle semantic from positional information during training.

### Why Sine and Cosine Together?

Because relative position can be expressed as a **linear transformation**. For any offset $k$:

$$\begin{pmatrix} PE_{(pos+k, 2i)} \\ PE_{(pos+k, 2i+1)} \end{pmatrix} = \begin{pmatrix} \cos(\omega_i k) & \sin(\omega_i k) \\ -\sin(\omega_i k) & \cos(\omega_i k) \end{pmatrix} \begin{pmatrix} PE_{(pos, 2i)} \\ PE_{(pos, 2i+1)} \end{pmatrix}$$

The rotation matrix depends only on the offset $k$, not on the absolute position. So "3 tokens ahead" looks the same whether we're at position 10 or 100. This lets the model learn relative positioning through linear operations.
If we would have absolute positioning, then during the scalar product we would encounter issues with this as different word pairs at different positions could have the same scalar product if one were to decouple the PE from them - meaning that the positional encoding would be senseless.

---

## How the QKV Attention Mechanism Works

The core idea is to figure out the **relationship between every token in the input sequence** — how much should token $i$ "pay attention" to token $j$ when building its new representation?

We do this by projecting each token's embedding $x_i$ into three vectors through learned weight matrices:

$$Q_i = W^Q x_i, \quad K_i = W^K x_i, \quad V_i = W^V x_i$$

- **Query** ($Q$) — what this token is looking for
- **Key** ($K$) — what this token offers to others
- **Value** ($V$) — the actual content this token contributes if attended to

The attention score between token $i$ and token $j$ is computed as the dot product of their query and key, scaled for numerical stability:

$$\text{score}_{i,j} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}$$

We then apply **softmax** over all $j$ to get a probability distribution (attention weights), and compute the new representation of token $i$ as a weighted sum of all value vectors:

$$\text{out}_i = \sum_j \text{softmax}\left(\frac{Q_i K_j^T}{\sqrt{d_k}}\right) \cdot V_j$$

In matrix form for the full sequence:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

### Concrete Example

Suppose the input sequence is: `"Shakespeare wrote Hamlet"`

| Token       | Query       | Key         | Value       |
|-------------|-------------|-------------|-------------|
| Shakespeare | $Q_1 = W^Q x_1$ | $K_1 = W^K x_1$ | $V_1 = W^V x_1$ |
| wrote       | $Q_2 = W^Q x_2$ | $K_2 = W^K x_2$ | $V_2 = W^V x_2$ |
| Hamlet      | $Q_3 = W^Q x_3$ | $K_3 = W^K x_3$ | $V_3 = W^V x_3$ |

When updating the token **"wrote"** ($i=2$), we compute its similarity to every token in the sequence:

$$\text{score}_{2,1} = Q_2 \cdot K_1 \quad \text{(wrote} \rightarrow \text{Shakespeare)}$$
$$\text{score}_{2,2} = Q_2 \cdot K_2 \quad \text{(wrote} \rightarrow \text{wrote, self-attention)}$$
$$\text{score}_{2,3} = Q_2 \cdot K_3 \quad \text{(wrote} \rightarrow \text{Hamlet)}$$

After softmax we get attention weights $a_{2,1},\ a_{2,2},\ a_{2,3}$ where $\sum_j a_{2,j} = 1$.

The new representation of "wrote" becomes:

$$\text{out}_2 = a_{2,1} V_1 + a_{2,2} V_2 + a_{2,3} V_3$$

Intuitively, we've moved the embedding of "wrote" to a new position in vector space — one that now encodes not just "wrote" in isolation, but its relationship to "Shakespeare" and "Hamlet". The attention weights determine how much each token influences this new position.

### Masking

Since this is a **decoder**, we apply a **causal mask** — token $i$ is not allowed to attend to any token $j > i$ (future tokens). This is done by adding $-\infty$ to those positions before the softmax, which zeroes them out after exponentiation, this is necessary for autoregressive text generation.

---

## Multi-Head Attention

Multi-Head Attention is essentially the same mechanism repeated `n_heads` times in parallel, each with its own $W^Q, W^K, W^V$ matrices of size $d_{model} \times d_k$ where $d_k = d_{model} / n_{heads}$.

Each head can independently learn to attend to different types of relationships in the sequence. The outputs of all heads are concatenated and passed through a final linear projection $W^O$ to mix the information across heads:

$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) \cdot W^O$$

---

## Feed-Forward Network

After attention, each position's embedding passes through the same FFN independently:

$$\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2$$

The inner dimension is $4 \times d_{model}$ (as in the paper). While attention mixes information *across* positions, the FFN processes each position *independently* — it essentially takes what was learned in the attention step and transforms it into a richer representation.

---

## LayerNorm

LayerNorm is applied after each sub-layer (Pre-LN variant here). Unlike BatchNorm which normalizes across the batch, LayerNorm normalizes across the feature dimension for each token independently:

$$\text{LN}(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta$$

where $\gamma$ and $\beta$ are learnable per-dimension scale and shift parameters.

---

## Text Generation

Generation is **autoregressive** — at each step we feed the current context into the model, take the logits of the last token, apply softmax with a temperature parameter, and sample the next token:

$$P(\text{next token}) = \text{softmax}\left(\frac{\text{logits}}{T}\right)$$

Higher temperature $T$ flattens the distribution (more random/creative output), lower temperature sharpens it (more conservative/repetitive). There's some similarity to statistical mechanics here (like the Ising Model). If the context exceeds `seq_len`, we truncate to the last `seq_len` tokens.

---

## Extending to Encoder-Decoder (for reference)

One could extend this to a full encoder-decoder setup — useful for tasks like translation (e.g. German → English). The encoder processes the source sequence with its own self-attention layers. The decoder then gets a third attention layer — **cross-attention** — where the encoder's output provides the $K$ and $V$ matrices, while the decoder's own representations provide $Q$. This forces the model to learn the mapping between the two sequences. The training target for the decoder remains the shifted-by-one token sequence, same as here.

Since we're only doing language modelling here, there is no cross-attention.